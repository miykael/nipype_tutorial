{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python for neuroimaging data - `Nilearn`\n",
    "\n",
    "The primary goal of this section is to become familiar with loading, modifying, saving, and visualizing neuroimaging data with Python. A secondary goal is to develop a conceptual understanding of the data structures involved, to facilitate diagnosing problems in data or analysis pipelines.\n",
    "\n",
    "To these ends, we'll be exploring two libraries: [nibabel](http://nipy.org/nibabel/) and [nilearn](https://nilearn.github.io/). Each of these projects has excellent documentation. While this should get you started, it is well worth your time to look through these sites.\n",
    "\n",
    "This notebook only covers nilearn, see the notebook [`tutorial_nibabel.ipynb`](tutorial_nibabel.ipynb) for more information about nibabel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "To make this notebook run, we need some data of subject 1. Use the following code to download all necessary data (about 35MB in total):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "datalad get -J4 /data/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_preproc.nii.gz \\\n",
    "                /data/ds000114/derivatives/freesurfer/sub-01/mri/aseg.mgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nilearn\n",
    "\n",
    "[Nilearn](http://nilearn.github.io/index.html) labels itself as: *A Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.*\n",
    "\n",
    "But it's much more than that. It is also an excellent library to **manipulate** (e.g. resample images, smooth images, ROI extraction, etc.) and **visulaize** your neuroimages.\n",
    "\n",
    "So let's visit all three of those domains:\n",
    "\n",
    "1. Image manipulation\n",
    "2. Image visualization\n",
    "3. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image settings\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image manipulation with `nilearn`\n",
    "\n",
    "### Let's create a mean image\n",
    "\n",
    "If you use nibabel to compute the mean image, you first need to load the img, get the data and then compute the mean thereof. With nilearn you can do all this in just one line with `mean image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image as nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nli.mean_img('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! What else can we do with the `image` module?  \n",
    "Let's see..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample image to a template\n",
    "Using `resample_to_img`, we can resample one image to have the same dimensions as another one. For example, let's resample an anatomical T1 image to the computed mean image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = nli.mean_img('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz')\n",
    "t1 = nli.load_img('/data/ds000114/sub-01/ses-test/anat/sub-01_ses-test_T1w.nii.gz')\n",
    "print([mean.shape, t1.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's resample the t1 image to the mean image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_t1 = nli.resample_to_img(t1, mean)\n",
    "resampled_t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image size of the resampled t1 image seems to be right. But what does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(t1, title='original t1', display_mode='z')\n",
    "plotting.plot_anat(resampled_t1, title='resampled t1', display_mode='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth an image\n",
    "Using `smooth_img`, we can very quickly smooth any kind of MRI image. Let's for example take the mean image from above and smooth it with different FWHM values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fwhm in range(1, 12, 5):\n",
    "    smoothed_img = nli.smooth_img(mean, fwhm)\n",
    "    plotting.plot_epi(smoothed_img, title=\"Smoothing %imm\" % fwhm,\n",
    "                     display_mode='z', cmap=plt.cm.magma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean an image to improve SNR\n",
    "\n",
    "Sometimes you also want to clean your functional images a bit to improve the SNR. For this, nilearn offers `clean_img`. You can improve the SNR of your fMRI signal by using one or more of the following options:\n",
    "\n",
    "- detrend\n",
    "- standardize\n",
    "- remove confounds\n",
    "- low- and high-pass filter\n",
    "\n",
    "**Note:** Low-pass filtering improves specificity. High-pass filtering should be kept small, to keep some sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load again a functional image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = nli.load_img('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz')\n",
    "TR = func.header['pixdim'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let's just detrend the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_d = nli.clean_img(func, detrend=True, standardize=False, t_r=TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and detrended timecourse of a random voxel\n",
    "x, y, z = [31, 14, 7]\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.transpose(func.get_data()[x, y, z, :]))\n",
    "plt.plot(np.transpose(func_d.get_data()[x, y, z, :]))\n",
    "plt.legend(['Original', 'Detrend']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what `standardize` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_ds = nli.clean_img(func, detrend=True, standardize=True, t_r=TR)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.transpose(func_d.get_data()[x, y, z, :]))\n",
    "plt.plot(np.transpose(func_ds.get_data()[x, y, z, :]))\n",
    "plt.legend(['Detrend', 'Detrend+standardize']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a final step, let's also remove the influence of the motion parameters from the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_ds_c = nli.clean_img(func, detrend=True, standardize=True, t_r=TR,\n",
    "                          confounds='scripts/sub-01_ses-test_task-fingerfootlips_bold_mcf.par')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(np.transpose(func_ds.get_data()[x, y, z, :]))\n",
    "plt.plot(np.transpose(func_ds_c.get_data()[x, y, z, :]))\n",
    "plt.legend(['Det.+stand.', 'Det.+stand.-confounds']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask an image and extract an average signal of a region\n",
    "\n",
    "Thanks to nibabel and nilearn you can consider your images just a special kind of a numpy array. Which means that you have all the liberties that you are used to.\n",
    "\n",
    "For example, let's take a functional image, (1) create the mean image thereof, than we (2) threshold it to only keep the voxels that have a value that is higher than 95% of all voxels. Of this thresholded image, we only (3) keep those regions that are bigger than 1000mm^3. And finally, we (4) binarize those regions to create a mask image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first, we load again a functional image and compute the mean thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = nli.mean_img('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `threshold_img` to only keep voxels that have a value that is higher than 95% of all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = nli.threshold_img(mean, threshold='99%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's only keep those voxels that are in regions/clusters that are bigger than 1000mm^3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_size = np.prod(thr.header['pixdim'][1:4])  # Size of 1 voxel in mm^3\n",
    "voxel_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the mask that only keeps those big clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.regions import connected_regions\n",
    "cluster = connected_regions(thr, min_region_size=1000. / voxel_size, smoothing_fwhm=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's binarize this cluster file to create a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = nli.math_img('np.mean(img,axis=3) > 0', img=cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us investigate this mask by visualizing it on the subject specific anatomy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = nli.load_img('/data/ds000114/sub-01/ses-test/anat/sub-01_ses-test_T1w.nii.gz')\n",
    "plotting.plot_stat_map(mask, bg_img=t1, display_mode='z', threshold=0.9, dim=-.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is now to take this mask, apply it to the original functional image and extract the mean of the temporal signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mask to original functional image\n",
    "from nilearn.masking import apply_mask\n",
    "\n",
    "all_timecourses = apply_mask(func, mask)\n",
    "all_timecourses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can bring the timecourses (or masked data) back into the original 3D/4D space with `unmask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.masking import unmask\n",
    "img_timecourse = unmask(all_timecourses, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mean trace of all extractet timecourses and plot the mean signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_timecourse = all_timecourses.mean(axis=1)\n",
    "plt.plot(mean_timecourse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis\n",
    "\n",
    "Nilearn gives you also the possibility to run an ICA on your data. This can either be on a single file or on multiple subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CanICA module\n",
    "from nilearn.decomposition import CanICA\n",
    "\n",
    "# Specify relevant parameters\n",
    "n_components = 5\n",
    "fwhm = 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify CanICA object\n",
    "canica = CanICA(n_components=n_components, smoothing_fwhm=fwhm,\n",
    "                threshold=3., verbose=10, random_state=0, n_jobs=-1,\n",
    "                standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "func = nli.load_img('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run/fit CanICA on input data\n",
    "canica.fit(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the independent components in brain space\n",
    "components_img = canica.masker_.inverse_transform(canica.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize those components on the T1 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nilearn.image import iter_img\n",
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "anat = nli.load_img('/data/ds000114/sub-01/ses-test/anat/sub-01_ses-test_T1w.nii.gz')\n",
    "\n",
    "for i, cur_img in enumerate(iter_img(components_img)):\n",
    "    plot_stat_map(cur_img, display_mode=\"z\", title=\"IC %d\" % i,\n",
    "                  cut_coords=5, colorbar=True, bg_img=anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The ICA components are not ordered, the visualization above and below therefore might look different from case to case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're now also curious about how those independent components, correlate with the functional image over time, you can use the following code to get some insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# Get data of the functional image\n",
    "orig_data = func.get_data()\n",
    "\n",
    "# Compute the pearson correlation between the components and the signal\n",
    "curves = np.array([[pearsonr(components_img.get_data()[..., j].ravel(),\n",
    "      orig_data[..., i].ravel())[0] for i in range(orig_data.shape[-1])]\n",
    "        for j in range(canica.n_components)])\n",
    "\n",
    "# Plot the components\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "plt.plot(curves.T)\n",
    "plt.legend(['IC %d' % i for i in range(canica.n_components)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Learning\n",
    "\n",
    "Recent work has shown that dictionary learning based techniques outperform ICA in term of stability and constitutes a better first step in a statistical analysis pipeline. Dictionary learning in neuro-imaging seeks to extract a few representative temporal elements along with their sparse spatial loadings, which constitute good extracted maps. Luckily, doing dictionary learning with `nilearn` is as easy as it can be.\n",
    "\n",
    "DictLearning is a ready-to-use class with the same interface as CanICA. Sparsity of output map is controlled by a parameter alpha: using a larger alpha yields sparser maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DictLearning module\n",
    "from nilearn.decomposition import DictLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dictionary learning object\n",
    "dict_learning = DictLearning(n_components=n_components, n_epochs=1,\n",
    "                             alpha=1., smoothing_fwhm=fwhm, standardize=True,\n",
    "                             verbose=1, random_state=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, let's fit the model to the data\n",
    "dict_learning.fit(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the independent components in brain space\n",
    "components_img = dict_learning.masker_.inverse_transform(dict_learning.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the 5 components\n",
    "for i, cur_img in enumerate(iter_img(components_img)):\n",
    "    plot_stat_map(cur_img, display_mode=\"z\", title=\"IC %d\" % i,\n",
    "                  cut_coords=5, colorbar=True, bg_img=anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps obtained with dictionary leaning are often easier to exploit as they are less noisy than ICA maps, with blobs usually better defined. Typically, smoothing can be lower than when doing ICA. While dictionary learning computation time is comparable to CanICA, obtained atlases have been shown to outperform ICA in a variety of classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# Get data of the functional image\n",
    "orig_data = func.get_data()\n",
    "\n",
    "# Compute the pearson correlation between the components and the signal\n",
    "curves = np.array([[pearsonr(components_img.get_data()[..., j].ravel(),\n",
    "      orig_data[..., i].ravel())[0] for i in range(orig_data.shape[-1])]\n",
    "        for j in range(dict_learning.n_components)])\n",
    "\n",
    "# Plot the components\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "plt.plot(curves.T)\n",
    "plt.legend(['IC %d' % i for i in range(dict_learning.n_components)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image visualization with `nilearn`\n",
    "\n",
    "Above, we've already seen some ways on how to visualize brain images with `nilearn`. And there are many more. To keep this notebook short, we will only take a look at some of them. For a complete list, see [nilearn's plotting section](http://nilearn.github.io/plotting/index.html).\n",
    "\n",
    "**Note:** In most of the `nilearn`'s plotting functions, you can specify the value `output_file=example.png'`, to save the figure directly to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data from Internet\n",
    "\n",
    "But before we can start, let's retrieve relevant datasets that we can use to plot. Luckily, nilearn comes with set of functions that download public data from Internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "localizer_dataset = datasets.fetch_localizer_button_task()\n",
    "localizer_tmap = localizer_dataset.tmaps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glass brain\n",
    "\n",
    "A really cool way to visualize your brain images on the MNI brain is nilearn's `plot_glass_brain()` function. It gives you a good overview of all significant voxels in our image.\n",
    "\n",
    "**Note**: It's important that your data is normalized to the MNI-template, as the overlay is otherwise not overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_glass_brain(localizer_tmap, threshold=3, colorbar=True,\n",
    "                          title='plot_glass_brain with display_mode=\"lyrz\"',\n",
    "                          plot_abs=False, display_mode='lyrz', cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay functional image onto anatomical image\n",
    "\n",
    "In this type of visualization, you can specify the axis through which you want to cut and the cut coordinates. `cut_coords` as integer 5 without a list implies that number of cuts in the slices should be maximum of 5.\n",
    "The coordinates to cut the slices are selected automatically. But you could also specify the exact cuts with`cut_coords=[-10, 0, 10, 20]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(localizer_tmap, display_mode='z', cut_coords=5, threshold=2,\n",
    "                       title=\"display_mode='z', cut_coords=5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `plot_stat_map()` can also be used to create figures with cuts in all directions, i.e. orthogonal cuts. For this, set `display_mode=ortho`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(localizer_tmap, display_mode='ortho', threshold=2,\n",
    "                       title=\"display_mode='orth', cut_coords=peak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay two images ontop of each other\n",
    "\n",
    "We can also create a plot that overlays different anatomical images. Let's show the FreeSurfer `aseg` segmentation over the T1 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi('/data/ds000114/derivatives/freesurfer/sub-01/mri/aseg.mgz',\n",
    "                  '/data/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_preproc.nii.gz',\n",
    "                  dim=-1, cut_coords=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that nilearn will accept an image or a filename equally. Also recall that `t1` was a NIfTI-1 image, while `aseg` is in a FreeSurfer `.mgz` file. Nilearn takes advantage of the common interface (data-affine-header) that nibabel provides for these different formats, and makes correctly aligned overlays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `add_edges` to see the overlay between two images\n",
    "\n",
    "Let's assume we want to see how well our anatomical image overlays with the mean functional image. Let's first load those two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn.image as nli\n",
    "t1 = nli.load_img('/data/ds000114/sub-01/ses-test/anat/sub-01_ses-test_T1w.nii.gz')\n",
    "mean = nli.mean_img('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `plot_anat` plotting function to plot the background image, in this case the mean fMRI image. Followed by the `add_edges` function to overlay the edges of the anatomical image onto the mean image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = plotting.plot_anat(mean, dim=-0.5)\n",
    "display.add_edges(t1, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Using the function `plot_epi()`, draw the image `mean` as a set of 5 slices spanning front to back. Suppress the background using the `vmin` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# Create solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "plotting.plot_epi(mean, display_mode='y', cut_coords=5, vmin=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Surface Plot\n",
    "\n",
    "One of the newest feature in nilearn is the possibility to project a 3D statistical map onto a cortical mesh using `nilearn.surface.vol_to_surf`. And than to display a surface plot of the projected map using `nilearn.plotting.plot_surf_stat_map`.\n",
    "\n",
    "**Note:** Both of those modules require that your `matplotlib` version is 1.3.1 or higher and that your `nilearn` version is 0.4 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make sure to download some example dataset to visualize\n",
    "from nilearn import datasets\n",
    "\n",
    "# Download an example statistical map\n",
    "localizer_dataset = datasets.fetch_localizer_button_task()\n",
    "localizer_tmap = localizer_dataset.tmaps[0]\n",
    "\n",
    "# Download the an example of a cortical mesh\n",
    "fsaverage = datasets.fetch_surf_fsaverage5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the statistical map from the volume onto the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import surface\n",
    "texture = surface.vol_to_surf(localizer_tmap, fsaverage.pial_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to plot the statistical map on the surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nilearn import plotting\n",
    "plotting.plot_surf_stat_map(fsaverage.infl_right, texture,\n",
    "                            hemi='right', title='Inflated surface - right hemisphere',\n",
    "                            threshold=1., bg_map=fsaverage.sulc_right,\n",
    "                            view='lateral', cmap='cold_hot')\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another example, using the pial surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_surf_stat_map(fsaverage.pial_left, texture,\n",
    "                            hemi='left',  title='Pial surface - left hemisphere',\n",
    "                            threshold=1., bg_map=fsaverage.sulc_left,\n",
    "                            view='medial', cmap='cold_hot')\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine learning with `nilearn`\n",
    "\n",
    "Although nilearn's visualizations are quite nice, its primary purpose was to facilitate machine learning in neuroimaging. It's in some sense the bridge between [nibabel](http://nipy.org/nibabel/) and [scikit-learn](http://scikit-learn.org/stable/). On the one hand, it reformats images to be easily passed to scikit-learn, and on the other, it reformats the results to produce valid nibabel images.\n",
    "\n",
    "So let's take a look at a short MVPA example.\n",
    "\n",
    "**Note**: This section is heavily based on the [nilearn decoding tutorial](https://nilearn.github.io/auto_examples/plot_decoding_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nilearn import datasets, plotting, input_data, image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example dataset\n",
    "\n",
    "Before we can do anything, we first need to download the example dataset. The whole dataset is almost 300MB big, and therefore might take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haxby_dataset = datasets.fetch_haxby(data_dir='data')\n",
    "\n",
    "bold = haxby_dataset.func[0]\n",
    "mask = haxby_dataset.mask_vt[0]\n",
    "anat = haxby_dataset.anat[0]\n",
    "labels = haxby_dataset.session_target[0]\n",
    "\n",
    "!nib-ls $bold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking and Un-masking data\n",
    "\n",
    "We need our functional data in a 2D, sample-by-voxel matrix. To get that, we'll select a set of voxels in VT cortex defined by `mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(mask, anat, cmap='Paired', dim=-.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NiftiMasker` is an object that applies a mask to a dataset and returns the masked voxels as a vector at each time point.\n",
    "`standardize=True` z-scores each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = input_data.NiftiMasker(mask_img=mask, standardize=True)\n",
    "samples = masker.fit_transform(bold)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its shape corresponds to the number of time-points times the number of voxels in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_epi = masker.inverse_transform(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the masked epi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_zscores = image.math_img(\"np.abs(img).max(axis=3)\", img=masked_epi)\n",
    "plotting.plot_stat_map(max_zscores, bg_img=anat, dim=-.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MVPA Example\n",
    "\n",
    "Multi-voxel pattern analysis (MVPA) is a general term for techniques that contrast conditions over multiple voxels. It's very common to use machine learning models to generate statistics of interest.\n",
    "\n",
    "In this case, we'll use the response patterns of voxels in VT cortex to predict the identity of the stimulus this subject was presented with. We'll use a support vector classifier (SVC) and leave-one-run-out cross-validation.\n",
    "\n",
    "**Note:** This section is not intended to teach machine learning, but to demonstrate a simple nilearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels file contains metadata for each volume, indicating the stimulus type and run number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 $labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `np.recfromcsv()`, we can refer to each column of this file by its header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = np.recfromcsv(labels, delimiter=\" \")\n",
    "attrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli, runs = attrs['labels'], attrs['chunks']\n",
    "print(np.unique(stimuli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's consider a two-class problem. Select the BOLD samples associated with bottles and shoes. We'll also need to select the corresponding stimuli and run numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mask = (stimuli == b'bottle') | (stimuli == b'shoe')\n",
    "\n",
    "samples_2class = samples[condition_mask]\n",
    "stimuli_2class = stimuli[condition_mask]\n",
    "runs_2class = runs[condition_mask]\n",
    "\n",
    "samples_2class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-run-out cross-validation trains on `(n - 1)` runs, and classifies the remaining run, for each run. Mean (across runs) cross-validation accuracy is a common statistic for classification-based MVPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "cva = cross_val_score(estimator=svc,\n",
    "                      X=samples_2class,\n",
    "                      y=stimuli_2class,\n",
    "                      groups=runs_2class,\n",
    "                      cv=LeaveOneGroupOut(),\n",
    "                      n_jobs=-1)\n",
    "print(cva, cva.mean(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to train a classifier on all of the data. This isn't useful for predicting, but we can read out the weight assigned to each voxel, giving a measure of its correlation with the stimulus type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(samples_2class, stimuli_2class)\n",
    "svc.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a value for each voxel, we can simply map this back to the volume using our `masker`, and visualize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_vol = masker.inverse_transform(svc.coef_)\n",
    "plotting.plot_stat_map(coef_vol, bg_img=anat, dim=-.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "In this section, we explored nilearn's tools for interfacing neuroimaging data and machine learning algorithms. Central to this is the concept of the masker, which moves data from 4-dimensional BOLD time series to a 2-dimensional series of feature vectors, and can map resulting statistics back into the original BOLD volume. We used leave-one-run-out cross-validation to explore 2-class support vector classification, and mapped feature weights back into the volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In the two notebooks about `nibabel` and `nilearn`, we've explored loading, saving and visualizing neuroimages, as well as how both packages can make some more sophisticated manipulations easy. At this point, you should be able to inspect and plot most images you encounter, as well as make modifications while preserving the alignment."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
